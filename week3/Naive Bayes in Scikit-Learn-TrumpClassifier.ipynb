{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes in Scikit-Learn: Trump Tweet Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article is a good introduction to NaiveBayes: \n",
    "* YHat post on Naives Bayes: http://blog.yhathq.com/posts/naive-bayes-in-python.html\n",
    "\n",
    "For scikit-learn, it's convenient to have the data files in separate directories based on their classification.\n",
    "So there's a \"positive\" and a \"negative\" folder in movies, or, say, an \"iphone\" and \"android\" for trump tweets. For training on reviews with scores, you might want a folder of 1-star reviews vs. a folder of 5-star reviews.\n",
    "\n",
    "We are using data from Trump tweets from David Robinson's analysis: http://varianceexplained.org/r/trump-tweets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp_utilities as mytools\n",
    "\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the data in your data/trump tweets folders, since it has more than we had in our small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want to load data into sklearn, be sure to provide the names of the subfolders for the categories.\n",
    "\n",
    "bunchf = load_files('data/trump/', categories=['android','iphone'], encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mandroid\u001b[m\u001b[m/ \u001b[34miphone\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data/trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DESCR', 'data', 'target', 'filenames', 'target_names'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what sklearn creates:\n",
    "\n",
    "bunchf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1385"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bunchf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/trump/iphone/I7.605198474616586e+17.txt',\n",
       "       'data/trump/iphone/I7.5247194141662e+17.txt',\n",
       "       'data/trump/android/A7.036017628763054e+17.txt',\n",
       "       'data/trump/iphone/I7.202473441978204e+17.txt',\n",
       "       'data/trump/iphone/I7.278983529652593e+17.txt',\n",
       "       'data/trump/android/A7.240390814208082e+17.txt',\n",
       "       'data/trump/iphone/I7.097367646137713e+17.txt',\n",
       "       'data/trump/android/A7.249115634051891e+17.txt',\n",
       "       'data/trump/iphone/I7.471948656335954e+17.txt',\n",
       "       'data/trump/iphone/I7.468950655917834e+17.txt'],\n",
       "      dtype='<U45')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf.filenames[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target'][0:10] # we can see that the 1 corresponds to iphone, 0 to android."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['android', 'iphone']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target_names']   # target names is just a list of the words for the 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the data into train and test using sklearn's cross-validation... now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "# try changing the random_state and % of test data - interesting differences in results.\n",
    "Xf_train, Xf_test, yf_train, yf_test = model_selection.train_test_split(bunchf.data, \n",
    "                                                                         bunchf.target, \n",
    "                                                                         test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instead of a simple true/false for a feature (word), we'll use the TF-IDF weight.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1246, 930)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sklearn vectorizer for TF-IDF has the stopwords as an option and a\n",
    "# lot of other features we can play with.\n",
    "# This is where I hacked around for a while trying to improve the results. You can too!\n",
    "\n",
    "tfidfvec = TfidfVectorizer(tokenizer=mytools.tokenize_clean,\n",
    "                           stop_words=[\"'s\", \"'m\", \"'s\", \"n't\", \"'d\", \"'ve\", \"'ll\", \"'re\"],\n",
    "                           #ngram_range=(1,2),\n",
    "                           max_df=0.75,\n",
    "                           max_features=20000,\n",
    "                           min_df=3)\n",
    "\n",
    "# we create the tf-idf model from the training data:\n",
    "vectors_train = tfidfvec.fit_transform(Xf_train)\n",
    "\n",
    "# Depending on whether you stemmed or lemmatized, you'll get different column numbers here!\n",
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75539568345323738"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# We set up our classifier\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "# We train the classifier on the training data and target classes (pos/neg)\n",
    "clf.fit(vectors_train, yf_train)\n",
    "\n",
    "# We use the model on the test data:\n",
    "vectors_test = tfidfvec.transform(Xf_test)\n",
    "# We get a prediction from the test data \n",
    "pred = clf.predict(vectors_test)\n",
    "# We check the accuracy against the \"truth\" in the yf_test var\n",
    "metrics.accuracy_score(yf_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting most informative features out of sklearn is a little ugly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code for binary classification case posted here: \n",
    "# http://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-11.8122\t\\the           \t\t-2.5509\thttps          \n",
      "\t-11.8122\ta.m.           \t\t-3.2237\tthank          \n",
      "\t-11.8122\tabc            \t\t-3.2491\ttrump2016      \n",
      "\t-11.8122\taction         \t\t-3.6585\tmakeamericagreatagain\n",
      "\t-11.8122\tagent          \t\t-4.3619\tgreat          \n",
      "\t-11.8122\tago            \t\t-4.4612\tamerica        \n",
      "\t-11.8122\tallowed        \t\t-4.6306\tamp            \n",
      "\t-11.8122\talong          \t\t-4.6651\tamericafirst   \n",
      "\t-11.8122\tangry          \t\t-4.7050\tnew            \n",
      "\t-11.8122\tanncoulter     \t\t-4.7546\tjoin           \n",
      "\t-11.8122\tanticipated    \t\t-4.7723\tmake           \n",
      "\t-11.8122\tanyone         \t\t-4.8030\thillary        \n",
      "\t-11.8122\tarena          \t\t-4.8305\tcrookedhillary \n",
      "\t-11.8122\taudit          \t\t-4.9135\tvotetrump      \n",
      "\t-11.8122\tbadly          \t\t-4.9668\timwithyou      \n",
      "\t-11.8122\tballot         \t\t-5.0513\tsoon           \n",
      "\t-11.8122\tbarbara        \t\t-5.2014\tclinton        \n",
      "\t-11.8122\tbertshad       \t\t-5.2391\ttonight        \n",
      "\t-11.8122\tbiggest        \t\t-5.2512\tsupport        \n",
      "\t-11.8122\tboard          \t\t-5.2657\ttrumppence16   \n",
      "\t-11.8122\tbosses         \t\t-5.2680\tpennsylvania   \n",
      "\t-11.8122\tbreitbart      \t\t-5.2706\tindiana        \n",
      "\t-11.8122\tbrexit         \t\t-5.2725\ttomorrow       \n",
      "\t-11.8122\tbrithume       \t\t-5.3105\tsee            \n",
      "\t-11.8122\tcandidates     \t\t-5.3757\ttrump          \n",
      "\t-11.8122\tcard           \t\t-5.3958\tpeople         \n",
      "\t-11.8122\tcareful        \t\t-5.4021\tsupertuesday   \n",
      "\t-11.8122\tcase           \t\t-5.4240\tcrooked        \n",
      "\t-11.8122\tchance         \t\t-5.4501\ttoday          \n",
      "\t-11.8122\tchicago        \t\t-5.4596\tenjoy          \n"
     ]
    }
   ],
   "source": [
    "# the results show the most negative features on the left side, and the most positive on the right.\n",
    "show_most_informative_features(tfidfvec, clf, n=30)  # 1 to the right, 0 to left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now use it to classify a tweet if we want:\n",
    "(Android is Trump, class 0, iphone is 1, not Trump)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(tfidfvec.transform([\"Proof she is a liar! Bad!\"]))   # android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(tfidfvec.transform([\"Join me at 7 and #makeamericangreat\"]))  # iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'android'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iphone'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target_names'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
