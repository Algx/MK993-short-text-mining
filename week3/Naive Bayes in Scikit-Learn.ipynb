{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes in Scikit-Learn: A Brief Intro\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article is a good introduction to NaiveBayes: \n",
    "* YHat post on Naives Bayes: http://blog.yhathq.com/posts/naive-bayes-in-python.html\n",
    "\n",
    "For scikit-learn, it's convenient to have the data files in separate directories based on their classification.\n",
    "So there's a \"positive\" and a \"negative\" folder in movies, or, say, an \"iphone\" and \"android\" for trump tweets. For training on reviews with scores, you might want a folder of 1-star reviews vs. a folder of 5-star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nlp_utilities as mytools\n",
    "\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command will not work on Windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSOURCE_README.txt\u001b[m\u001b[m \u001b[31mall_pos.txt\u001b[m\u001b[m       \u001b[30m\u001b[43mnegative\u001b[m\u001b[m\r\n",
      "\u001b[31mall_neg.txt\u001b[m\u001b[m       \u001b[30m\u001b[43mallfiles\u001b[m\u001b[m          \u001b[30m\u001b[43mpositive\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/movie_reviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the data in your nltk_data/copora/movie_reviews folder, since it has more than we had in our small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you want to load data into sklearn, be sure to provide the names of the subfolders for the categories.\n",
    "\n",
    "bunchf = load_files('data/movie_reviews/', categories=['positive','negative'], encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'filenames', 'data', 'DESCR', 'target_names'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what sklearn creates:\n",
    "\n",
    "bunchf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bunchf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/movie_reviews/negative/cv696_tok-28821.txt',\n",
       "       'data/movie_reviews/positive/cv675_tok-11864.txt',\n",
       "       'data/movie_reviews/positive/cv699_tok-10425.txt',\n",
       "       'data/movie_reviews/negative/cv698_tok-20916.txt',\n",
       "       'data/movie_reviews/negative/cv681_tok-11979.txt',\n",
       "       'data/movie_reviews/negative/cv672_tok-20564.txt',\n",
       "       'data/movie_reviews/positive/cv674_tok-11591.txt',\n",
       "       'data/movie_reviews/positive/cv698_tok-27735.txt',\n",
       "       'data/movie_reviews/positive/cv680_tok-18142.txt',\n",
       "       'data/movie_reviews/negative/cv692_tok-4797.txt'],\n",
       "      dtype='<U47')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['filenames'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bunchf['target_names']  # 0 is negative, 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bunchf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the data into train and test using sklearn's cross-validation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# try changing the random_state and % of test data - interesting differences in results.\n",
    "Xf_train, Xf_test, yf_train, yf_test = model_selection.train_test_split(bunchf.data, \n",
    "                                                                         bunchf.target, \n",
    "                                                                         test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instead of a simple true/false for a feature (word), we'll use the TF-IDF weight.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 1271)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The sklearn vectorizer for TF-IDF has the stopwords as an option and a\n",
    "# lot of other features we can play with.\n",
    "# This is where I hacked around for a while trying to improve the results. You can too!\n",
    "\n",
    "tfidfvec = TfidfVectorizer(tokenizer=mytools.tokenize_clean,\n",
    "                           stop_words=[\"'s\", \"'m\", \"'s\", \"n't\", \"'d\", \"'ve\", \"'t\", \"'ll\", \"'re\"],\n",
    "                           ngram_range=(1,2),\n",
    "                           max_df=0.80,\n",
    "                           #max_features=20000,\n",
    "                           min_df=3)\n",
    "\n",
    "# we create the tf-idf model from the training data:\n",
    "vectors_train = tfidfvec.fit_transform(Xf_train)\n",
    "\n",
    "# Depending on whether you stemmed or lemmatized, you'll get different column numbers here!\n",
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# We set up our classifier\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "\n",
    "# We train the classifier on the training data and target classes (pos/neg)\n",
    "clf.fit(vectors_train, yf_train)\n",
    "\n",
    "# We use the model on the test data:\n",
    "vectors_test = tfidfvec.transform(Xf_test)\n",
    "\n",
    "# We get a prediction from the test data \n",
    "pred = clf.predict(vectors_test)\n",
    "# We check the accuracy against the \"truth\" in the yf_test var\n",
    "metrics.accuracy_score(yf_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting most informative features out of sklearn is a little ugly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code for binary classification case posted here: \n",
    "# http://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-10.2393\taction sequences\t\t-5.1675\tmovie          \n",
      "\t-10.2393\tadventure      \t\t-5.3990\tlike           \n",
      "\t-10.2393\tair            \t\t-5.4937\tjackie         \n",
      "\t-10.2393\tal             \t\t-5.5858\tgood           \n",
      "\t-10.2393\talmost every   \t\t-5.6310\tdamon          \n",
      "\t-10.2393\tarchived       \t\t-5.6580\tsee            \n",
      "\t-10.2393\tbadly          \t\t-5.6648\ttime           \n",
      "\t-10.2393\tbattle         \t\t-5.6978\tshow           \n",
      "\t-10.2393\tbeautiful      \t\t-5.7301\talso           \n",
      "\t-10.2393\tbrad           \t\t-5.8238\tdouglas        \n",
      "\t-10.2393\tcentral        \t\t-5.8428\twife           \n",
      "\t-10.2393\tcharacter much \t\t-5.8492\tnew            \n",
      "\t-10.2393\tcinematography \t\t-5.8654\tfirst          \n",
      "\t-10.2393\tclark          \t\t-5.8856\tway            \n",
      "\t-10.2393\tcrap           \t\t-5.8989\tgreat          \n",
      "\t-10.2393\tcritical       \t\t-5.9013\tperformances   \n",
      "\t-10.2393\tcry            \t\t-5.9044\ttwo            \n",
      "\t-10.2393\tcut            \t\t-5.9107\tplot           \n",
      "\t-10.2393\tcute           \t\t-5.9191\teven           \n",
      "\t-10.2393\tdavis          \t\t-5.9248\tmuch           \n",
      "\t-10.2393\tdesperately    \t\t-5.9332\tstory          \n",
      "\t-10.2393\tdirecting      \t\t-5.9337\tcharacter      \n",
      "\t-10.2393\tdistributor    \t\t-5.9453\tlittle         \n",
      "\t-10.2393\tdrop           \t\t-5.9587\tmusic          \n",
      "\t-10.2393\tdull           \t\t-5.9636\tback           \n",
      "\t-10.2393\tdumb           \t\t-5.9733\tlook           \n",
      "\t-10.2393\teffort         \t\t-5.9741\taffleck        \n",
      "\t-10.2393\telement        \t\t-5.9866\twilliams       \n",
      "\t-10.2393\tencounter      \t\t-5.9905\tjohn           \n",
      "\t-10.2393\tentry          \t\t-6.0014\tmakes          \n",
      "\t-10.2393\texamples       \t\t-6.0165\tcharacters     \n",
      "\t-10.2393\texercise       \t\t-6.0207\twould          \n",
      "\t-10.2393\tfake           \t\t-6.0212\tus             \n",
      "\t-10.2393\tfirst minutes  \t\t-6.0308\tmany           \n",
      "\t-10.2393\tfirst time     \t\t-6.0309\tgo             \n",
      "\t-10.2393\tforce          \t\t-6.0389\tmade           \n",
      "\t-10.2393\thappy          \t\t-6.0435\tanother        \n",
      "\t-10.2393\thitchcock      \t\t-6.0441\tsay            \n",
      "\t-10.2393\thorrible       \t\t-6.0455\tman            \n",
      "\t-10.2393\thtml           \t\t-6.0503\tget            \n",
      "\t-10.2393\timpression     \t\t-6.0529\tcomedy         \n",
      "\t-10.2393\tincludes       \t\t-6.0588\tlee            \n",
      "\t-10.2393\tindian         \t\t-6.0618\tscene          \n",
      "\t-10.2393\tinvolved       \t\t-6.0721\tcould          \n",
      "\t-10.2393\tkilling        \t\t-6.0736\twell           \n",
      "\t-10.2393\tlacks          \t\t-6.0775\tyears          \n",
      "\t-10.2393\tlatest         \t\t-6.0961\tstill          \n",
      "\t-10.2393\tlearn          \t\t-6.0976\tevery          \n",
      "\t-10.2393\tlength         \t\t-6.1006\tdone           \n",
      "\t-10.2393\tlethal         \t\t-6.1067\taudience       \n",
      "\t-10.2393\tmake movie     \t\t-6.1072\tfilms          \n",
      "\t-10.2393\tmarry          \t\t-6.1084\tsmith          \n",
      "\t-10.2393\tmess           \t\t-6.1091\tthree          \n",
      "\t-10.2393\tmoronic        \t\t-6.1184\tput            \n",
      "\t-10.2393\tmotion picture \t\t-6.1213\ttom            \n",
      "\t-10.2393\tmovie bad      \t\t-6.1275\tlife           \n",
      "\t-10.2393\tmpaa           \t\t-6.1303\tdinner         \n",
      "\t-10.2393\tnudity         \t\t-6.1322\tgame           \n",
      "\t-10.2393\topportunity    \t\t-6.1358\tever           \n",
      "\t-10.2393\tpatrick        \t\t-6.1435\tmichael        \n"
     ]
    }
   ],
   "source": [
    "# the results show the most negative features on the left side, and the most positive on the right.\n",
    "show_most_informative_features(tfidfvec, clf, n=60)  # positive to the right, negative to left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now use it to classify a review if we want (negative reviews are 0, positive are 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(tfidfvec.transform([\"This movie was very bad. But I loved the characters and plot.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(tfidfvec.transform([\"This movie was very good. But I loved the characters and plot.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
