{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to low level NLP - Tokenization, Stopwords, Frequencies, Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to stop and install some stuff for NLTK to work properly, so let me know when you get an error using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "PUNCTUATION = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Read in a file to use for practice.  The directory is one level above us now, in data/books.  You can add other files into the data directory if you want.\n",
    "\n",
    "** This command will not work on Windows:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mAusten_Emma.txt\u001b[m\u001b[m       \u001b[31mMelville_MobyDick.txt\u001b[m\u001b[m\r\n",
      "\u001b[31mAusten_Pride.txt\u001b[m\u001b[m      \u001b[31mlovecraft.txt\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/books/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/books/Austen_Emma.txt\", errors=\"ignore\") as handle:\n",
    "    text = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMMA\\nBY\\nJANE AUSTEN\\nVOLUME I\\nCHAPTER I\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home and\\nhappy disposition, seemed to unite some of the best blessings of\\nexistence; and had lived'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## if you don't want the newlines in there - replace them all with a space!\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EMMA BY JANE AUSTEN VOLUME I CHAPTER I Emma Woodhouse, handsome, clever, and rich, with a comfortabl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to use some features of the NLTK module, you need to download and install some things from it.\n",
    "\n",
    "Tip: Some people have this little UI freeze up on them if they try to scroll using their mousepad instead of the scrollbar.  If this happens, restart your notebook kernel (see the menu \"kernel\" on top).  You will have to run the cells above again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look for the \"python window\" that was opened on your machine.  \n",
    "\n",
    "* In the Corpora tab you need to get \"stopwords.\"\n",
    "* In the Models tab you need \"Punkt Tokenizer Models.\"\n",
    "* In Models you also need \"Averaged Perceptron Tagger.\"\n",
    "\n",
    "<img src=\"assets/nltk_downloader.png\">\n",
    "\n",
    "### After you download these files, you must close the dialog using the X in the upper left corner, in order to get back to the notebook cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of breaking up a text into smaller pieces that you can count (or analyze) is called **\"tokenizing.\"**  Here we break it up into sentences using a sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She dearly loved her father, but he was no companion for her.',\n",
       " 'He could not meet her in conversation, rational or playful.',\n",
       " 'The evil of the actual disparity in their ages (and Mr. Woodhouse had not married early) was much increased by his constitution and habits; for having been a valetudinarian all his life, without activity of mind or body, he was a much older man in ways than in years; and though everywhere beloved for the friendliness of his heart and his amiable temper, his talents could not have recommended him at any time.',\n",
       " 'Her sister, though comparatively but little removed by matrimony, being settled in London, only sixteen miles off, was much beyond her daily reach; and many a long October and November evening must be struggled through at Hartfield, before Christmas brought the next visit from Isabella and her husband, and their little children, to fill the house, and give her pleasant society again.',\n",
       " 'Highbury, the large and populous village, almost amounting to a town, to which Hartfield, in spite of its separate lawn, and shrubberies, and name, did really belong, afforded her no equals.',\n",
       " 'The Woodhouses were first in consequence there.',\n",
       " 'All looked up to them.',\n",
       " 'She had many acquaintance in the place, for her father was universally civil, but not one among them who could be accepted in lieu of Miss Taylor for even half a day.',\n",
       " 'It was a melancholy change; and Emma could not but sigh over it, and wish for impossible things, till her father awoke, and made it necessary to be cheerful.',\n",
       " 'His spirits required support.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Breaking it up by sentence! \n",
    "sentences = nltk.sent_tokenize(text)\n",
    "sentences[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many sentences?\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Word\" tokenization** is much more common and useful.  But there are lots of types of word tokenization, depending on how you handle punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " ',',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'unite',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'of',\n",
       " 'existence',\n",
       " ';',\n",
       " 'and',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty-one',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'with',\n",
       " 'very',\n",
       " 'little',\n",
       " 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens[20:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy',\n",
       " 'disposition',\n",
       " ',',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'unite',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'of',\n",
       " 'existence',\n",
       " ';',\n",
       " 'and',\n",
       " 'had',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty',\n",
       " '-',\n",
       " 'one',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'with',\n",
       " 'very']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the difference here:\n",
    "nltk.wordpunct_tokenize(text)[23:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other options for tokenization in NLTK.  You can test some out here: http://text-processing.com/demo/tokenize/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words (First Attempt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's break up a text and count some stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMMA',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(text)\n",
    "tokens[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191739"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how many tokens we have:\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's one way we learned to count things?  Using the Counter from collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 12016),\n",
       " ('.', 6359),\n",
       " ('to', 5124),\n",
       " ('the', 4842),\n",
       " ('and', 4652),\n",
       " ('of', 4272),\n",
       " ('I', 3164),\n",
       " ('--', 3097),\n",
       " ('a', 3001),\n",
       " ('was', 2383),\n",
       " ('her', 2360),\n",
       " (';', 2353),\n",
       " ('not', 2242),\n",
       " (\"''\", 2189),\n",
       " ('in', 2103),\n",
       " ('it', 2101),\n",
       " ('``', 1998),\n",
       " ('be', 1965),\n",
       " ('she', 1774),\n",
       " ('that', 1728)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing with another book.  Let's make a function to make this easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_common(filename):\n",
    "    # Takes a path to a file, splits it up into tokens, and prints the \"count\" most common.\n",
    "    from collections import Counter\n",
    "    text = None\n",
    "    mycounts = None\n",
    "    \n",
    "    with open(filename, errors=\"ignore\") as handle:\n",
    "        text = handle.read()\n",
    "    if text:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        words = nltk.word_tokenize(text)\n",
    "        mycounts = Counter(words)\n",
    "    return mycounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call this with a path to your book file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = get_most_common(\"data/books/Austen_Pride.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\t9117\n",
      ".\t5034\n",
      "to\t4081\n",
      "the\t4047\n",
      "of\t3588\n",
      "and\t3377\n",
      "her\t2139\n",
      "I\t2049\n",
      "a\t1891\n",
      "was\t1841\n",
      "in\t1778\n",
      "``\t1770\n",
      "''\t1739\n",
      ";\t1538\n",
      "that\t1514\n"
     ]
    }
   ],
   "source": [
    "if counts:\n",
    "    for word,count in counts.most_common(15):\n",
    "        print(\"%s\\t%s\" % (word,count))\n",
    "else:\n",
    "    print(\"Something wrong with your path, no file read, so no word counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t734\n",
      ",\t650\n",
      "of\t440\n",
      "and\t439\n",
      ".\t329\n",
      "a\t248\n",
      "to\t218\n",
      "in\t215\n",
      "I\t176\n",
      "was\t155\n"
     ]
    }
   ],
   "source": [
    "counts = get_most_common(\"data/books/lovecraft.txt\")\n",
    "for word,count in counts.most_common(10):\n",
    "        print(\"%s\\t%s\" % (word,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\t18923\n",
      "the\t13522\n",
      ".\t7064\n",
      "of\t6402\n",
      "and\t5931\n",
      "a\t4465\n",
      "to\t4444\n",
      ";\t4143\n",
      "in\t3830\n",
      "that\t2942\n"
     ]
    }
   ],
   "source": [
    "counts = get_most_common(\"data/books/Melville_MobyDick.txt\")\n",
    "for word,count in counts.most_common(10):\n",
    "        print(\"%s\\t%s\" % (word,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These aren't very interesting top words.  Very different books look quite similar when you look at the most common tokens. The problem is that these are common words and punctuation, and they appear in all books the most frequently. You will read about Zipf's law in the reading this week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 2 things we need to do to clean the word list.  One is remove punctuation, the other is remove stopwords.**\n",
    "\n",
    "**Let's use a list comprehension to remove the punctuation first.  We can get a list of punctuation from the string library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[4:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice this is a string of characters - see the quotes around it?\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191739"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember we can do this with strings to test if they are substrings:\n",
    "'?' in punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a\" in punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this says: make my new list out of the elements in tokens if they aren't in punctuation.\n",
    "no_punct = [word for word in tokens if word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'a']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punct[4:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22936"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) - len(no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's make a function.  I am assuming punctuation is a global variable, defined outside.\n",
    "# Frequently globals and constants are assigned at the top of your file with capital letters:\n",
    "\n",
    "import string\n",
    "\n",
    "PUNCTUATION = string.punctuation\n",
    "\n",
    "def remove_punct(wordlist):\n",
    "    return [word for word in wordlist if word not in PUNCTUATION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's edit our function for the most common words and add this into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's put printing inside the function right now:\n",
    "def print_most_common(filename, count=10):\n",
    "    # Takes a path to a file, splits it up into tokens, and prints the \"count\" most common.\n",
    "    # This version removes punctuation.\n",
    "    from collections import Counter\n",
    "    text = None\n",
    "    mycounts = None\n",
    "    \n",
    "    try:\n",
    "        with open(filename, errors=\"ignore\") as handle:\n",
    "            text = handle.read()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            words = nltk.word_tokenize(text)\n",
    "            \n",
    "            print(\"Total Length\", len(words))\n",
    "            words = remove_punct(words)  # new stuff!\n",
    "            print(\"Total Length After Punct Removal\", len(words))\n",
    "            \n",
    "            mycounts = Counter(words)\n",
    "            print(\"Word\\tCount\")\n",
    "            for word,count in mycounts.most_common(count):\n",
    "                print(\"%s\\t%s\" % (word,count))\n",
    "    except:\n",
    "        print(\"Something is wrong with your file location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length 191739\n",
      "Total Length After Punct Removal 168803\n",
      "Word\tCount\n",
      "to\t5124\n",
      "the\t4842\n",
      "and\t4652\n",
      "of\t4272\n",
      "I\t3164\n",
      "--\t3097\n",
      "a\t3001\n",
      "was\t2383\n",
      "her\t2360\n",
      "not\t2242\n",
      "''\t2189\n",
      "in\t2103\n",
      "it\t2101\n",
      "``\t1998\n",
      "be\t1965\n"
     ]
    }
   ],
   "source": [
    "print_most_common(\"data/books/Austen_Emma.txt\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length 12025\n",
      "Total Length After Punct Removal 10912\n",
      "Word\tCount\n",
      "the\t734\n",
      "of\t440\n",
      "and\t439\n",
      "a\t248\n",
      "to\t218\n",
      "in\t215\n",
      "I\t176\n",
      "was\t155\n",
      "had\t131\n",
      "that\t128\n",
      "which\t95\n",
      "my\t93\n",
      "with\t93\n",
      "it\t90\n",
      "--\t85\n"
     ]
    }
   ],
   "source": [
    "print_most_common(\"data/books/lovecraft.txt\",15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happened there? Any theories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stopwords\" are words that are usually excluded because they are common connectors (or determiners, or short verbs) that are not considered to carry meaning. **BEWARE**: Always check stopword lists to see if you agree with their contents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice they are lowercase.  This means we need to be sure we lowercase our text if we want to match against them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_stops = stopwords.words('french')\n",
    "french_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How would we get the french stopwords and look at them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stopwords efficiently, we need to make sure everything is lower case. This will also mean our counts are better in general. Words at the beginning of a sentence are still the same as words in the middle, just capitalized.  If we want good counts, they must all look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lowercase = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austen'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'by',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volume',\n",
       " 'i',\n",
       " 'chapter',\n",
       " 'i',\n",
       " 'emma',\n",
       " 'woodhouse']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191739 191739\n"
     ]
    }
   ],
   "source": [
    "print(len(lowercase), len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104376"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try this without .lower() in the if-statement and check the size!\n",
    "# We are using a python list comprehension to remove the tokens from Emma (after lowercasing them!) that are stopwords\n",
    "nostops = [token for token in lowercase if token not in english_stops]\n",
    "len(nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volume',\n",
       " 'chapter',\n",
       " 'emma',\n",
       " 'woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'rich',\n",
       " ',',\n",
       " 'comfortable']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now look at the first 15 words:\n",
    "nostops[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def remove_stops(wordlist, stopwords):\n",
    "    # takes a list of words and stopwords and filters out stopwords after lowercasing all.\n",
    "    lowercase = [word.lower() for word in wordlist]\n",
    "    return [word for word in lowercase if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_stops(tokens, english_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our function again to remove stopwords along with making it all lowercase and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's rename it again\n",
    "def print_most_common3(filename, stops, count=10):\n",
    "    # Takes a path to a file, stopwords list, splits it up into tokens, and prints the \n",
    "    # \"count\" most common.  Count will default to 10 if not specified.\n",
    "    # This version removes punctuation.\n",
    "    from collections import Counter\n",
    "    text = None\n",
    "    mycounts = None\n",
    "    try:\n",
    "        with open(filename, errors=\"ignore\") as handle:\n",
    "            text = handle.read()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            words = nltk.word_tokenize(text)\n",
    "            \n",
    "            print(\"Length Before Cleaning\", len(words))\n",
    "            words = remove_punct(words)\n",
    "            words = remove_stops(words, stops)\n",
    "            print(\"Length After Cleaning\", len(words))\n",
    "            \n",
    "            mycounts = Counter(words)\n",
    "            print(\"Word\\tCount\")\n",
    "            for word,count in mycounts.most_common(count):\n",
    "                print(\"%s\\t%s\" % (word,count))\n",
    "    except:\n",
    "        print(\"Something is wrong with your file location.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Before Cleaning 191739\n",
      "Something is wrong with your file location.\n"
     ]
    }
   ],
   "source": [
    "print_most_common3(\"data/books/Austen_Emma.txt\", english_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Before Cleaning 250560\n",
      "Something is wrong with your file location.\n"
     ]
    }
   ],
   "source": [
    "# in this call, we specify the number for count.\n",
    "print_most_common3(\"data/books/Melville_MobyDick.txt\", english_stops, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a little function to allow us to remove any string we want, too.  This is basically a custom stopwords filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_custom(wordlist, mylist):\n",
    "    return [word for word in wordlist if word not in mylist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of words to also remove might start like this:\n",
    "TO_REMOVE = [\"--\",\"``\",\"'s\", \"''\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'fred']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test it out:\n",
    "remove_custom(['hi', '--', 'fred'], TO_REMOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So how would we use this and the other removal and cleaning functions?\n",
    "\n",
    "In our functions, we are just doing things to our words list, one after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mywords = ['Hi', 'there', ',', 'Sally', '--', 'this', 'is', 'the', 'FBI', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', 'Sally', '--', 'this', 'is', 'the', 'FBI']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = remove_punct(mywords)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'sally', '--', 'fbi']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = remove_stops(words, STOPS)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'sally', 'fbi']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = remove_custom(words, TO_REMOVE)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we use it in our existing function?  We might need to change the arguments we call this with, to pass in the list of custom stopwords.  Edit this in class to add remove_custom and handle the new list of custom stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's rename it again\n",
    "def print_most_common4(filename, stopwords, customstops=None, count=10):\n",
    "    # Takes a path to a file, splits it up into tokens, and prints the \"count\" most common.\n",
    "    # This version removes punctuation.\n",
    "    from collections import Counter\n",
    "    text = None\n",
    "    mycounts = None\n",
    "    if not customstops:\n",
    "        customstops = []\n",
    "        \n",
    "    try:\n",
    "        with open(filename, errors=\"ignore\") as handle:\n",
    "            text = handle.read()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            words = nltk.word_tokenize(text)\n",
    "            \n",
    "            words = remove_punct(words)\n",
    "            words = remove_stops(words, stopwords)\n",
    "            words = remove_custom(words, customstops)\n",
    "            \n",
    "            mycounts = Counter(words)\n",
    "            print(\"Word\\tCount\")\n",
    "            for word,count in mycounts.most_common(count):\n",
    "                print(\"%s\\t%s\" % (word,count))\n",
    "    except:\n",
    "        print(\"Something is wrong with your file location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now when you call that, you should be able to see a list you can live with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tCount\n",
      "mr.\t1089\n",
      "emma\t860\n",
      "could\t836\n",
      "would\t818\n",
      "mrs.\t668\n",
      "miss\t597\n",
      "must\t566\n",
      "harriet\t500\n",
      "much\t484\n",
      "said\t483\n",
      "one\t447\n",
      "weston\t437\n",
      "every\t434\n",
      "thing\t394\n",
      "elton\t384\n"
     ]
    }
   ],
   "source": [
    "# here's how we might call this after editing it... (this is a hint)\n",
    "print_most_common4(\"data/books/Austen_Emma.txt\", english_stops, customstops=TO_REMOVE, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tCount\n",
      "house\t59\n",
      "street\t41\n",
      "uncle\t39\n",
      "one\t35\n",
      "harris\t34\n",
      "could\t29\n",
      "seemed\t27\n",
      "would\t22\n",
      "might\t21\n",
      "door\t20\n",
      "thing\t18\n",
      "shunned\t18\n",
      "place\t17\n",
      "cellar\t17\n",
      "time\t16\n"
     ]
    }
   ],
   "source": [
    "# here's how we might call this after editing it... (this is a hint)\n",
    "print_most_common4(\"data/books/lovecraft.txt\", STOPS, customstops=TO_REMOVE, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tCount\n",
      "whale\t1027\n",
      "one\t899\n",
      "like\t572\n",
      "upon\t560\n",
      "ahab\t511\n",
      "man\t496\n",
      "ship\t464\n",
      "old\t439\n",
      "ye\t433\n",
      "would\t430\n",
      "though\t380\n",
      "sea\t367\n",
      "yet\t344\n",
      "time\t324\n",
      "captain\t323\n"
     ]
    }
   ],
   "source": [
    "# here's how we might call this after editing it... (this is a hint)\n",
    "print_most_common4(\"data/books/Melville_MobyDick.txt\", english_stops, customstops=TO_REMOVE, count=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to read about Python functions, positional arguments, and keyword-named arguments here: http://sys-exit.blogspot.fr/2013/07/python-positional-arguments-and-keyword.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a bunch of separate little functions now to pull apart the cleaning from the printing.  We might want to use that list of words for more than just a print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(path):\n",
    "    # This takes a file path and word_tokenizes it, returning a list of words.\n",
    "    words = None\n",
    "    try:\n",
    "        with open(path, errors=\"ignore\") as handle:\n",
    "            text = handle.read()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            words = nltk.word_tokenize(text)\n",
    "            return words\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_counts(tokens, count=10):\n",
    "    # Takes a list of words, counts, prints top \"count\" words.\n",
    "    from collections import Counter\n",
    "    mycounts = Counter(tokens)\n",
    "    print(\"Word\\tCount\")\n",
    "    for word,count in mycounts.most_common(count):\n",
    "        print(\"%s\\t%s\" % (word,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's define a small python function that's a pretty common one for text processing.\n",
    "\n",
    "def clean_tokens(tokens, stops):\n",
    "    \"\"\" Lowercases, takes out punct and stopwords and short strings. \"\"\"\n",
    "    words = remove_punct(tokens)\n",
    "    words = remove_stops(words, stops)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMMA',\n",
       " 'BY',\n",
       " 'JANE',\n",
       " 'AUSTEN',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = tokenize_text(\"data/books/Austen_Emma.txt\")\n",
    "emma[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volume',\n",
       " 'chapter',\n",
       " 'emma',\n",
       " 'woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'rich']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_clean = clean_tokens(emma, english_stops)\n",
    "emma_clean[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81440"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emma_cleaner = remove_custom(emma_clean, TO_REMOVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volume',\n",
       " 'chapter',\n",
       " 'emma',\n",
       " 'woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'rich',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " 'seemed',\n",
       " 'unite',\n",
       " 'best',\n",
       " 'blessings',\n",
       " 'existence',\n",
       " 'lived',\n",
       " 'nearly',\n",
       " 'twenty-one',\n",
       " 'years',\n",
       " 'world',\n",
       " 'little',\n",
       " 'distress',\n",
       " 'vex',\n",
       " 'youngest',\n",
       " 'two',\n",
       " 'daughters',\n",
       " 'affectionate',\n",
       " 'indulgent',\n",
       " 'father',\n",
       " 'consequence',\n",
       " 'sister',\n",
       " 'marriage',\n",
       " 'mistress',\n",
       " 'house',\n",
       " 'early',\n",
       " 'period']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma_cleaner[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73230"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emma_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\tCount\n",
      "mr.\t1089\n",
      "emma\t860\n",
      "could\t836\n",
      "would\t818\n",
      "mrs.\t668\n",
      "miss\t597\n",
      "must\t566\n",
      "harriet\t500\n",
      "much\t484\n",
      "said\t483\n"
     ]
    }
   ],
   "source": [
    "print_counts(emma_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you do now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Word Frequencies Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way to do this is with nltk.FreqDist, which creates an object with keys that are \n",
    "# the vocabulary, and values for the counts- just like a counter.\n",
    "\n",
    "frequencies = nltk.FreqDist(emma_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies['mr.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mr.', 1089),\n",
       " ('emma', 860),\n",
       " ('could', 836),\n",
       " ('would', 818),\n",
       " ('mrs.', 668),\n",
       " ('miss', 597),\n",
       " ('must', 566),\n",
       " ('harriet', 500),\n",
       " ('much', 484),\n",
       " ('said', 483)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary size - unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequencies.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to save the words and counts to a file to use, you can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordpairs = frequencies.most_common()\n",
    "with open(\"emma_word_counts.csv\", \"w\") as handle:\n",
    "    for pair in wordpairs:\n",
    "        handle.write(pair[0] + \",\" + str(pair[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr.,1089\r\n",
      "emma,860\r\n",
      "could,836\r\n",
      "would,818\r\n",
      "mrs.,668\r\n"
     ]
    }
   ],
   "source": [
    "# this won't work on Windows - it's a unix command to look at the top of the file.\n",
    "!head -n5 emma_word_counts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How would we read that into pandas and make a graph of the top 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Common Pairs of Words (\"Bigrams\")\n",
    "\n",
    "Words occur in common sequences, sometimes.  We call word pairs \"bigrams\" (and word triples \"trigrams\").  We refer to N-grams when we mean \"sequences of some length.\"  These functions work on the tokenized text -- the list of tokens.  If we clean it first (punctuation and stopword removal), then the bi- and tri-grams may not look like good grammar.  But it may still be useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mr.', 'knightley'), 1946.6592454307174),\n",
       " (('mrs.', 'weston'), 1850.336542407057),\n",
       " (('frank', 'churchill'), 1606.6422087377366),\n",
       " (('mr.', 'elton'), 1322.480745223186),\n",
       " (('miss', 'woodhouse'), 1252.7696197262974),\n",
       " (('miss', 'bates'), 938.7056563680686),\n",
       " (('miss', 'fairfax'), 889.3547077244223),\n",
       " (('every', 'body'), 885.9210086539515),\n",
       " (('jane', 'fairfax'), 865.7102826733177),\n",
       " (('mrs.', 'elton'), 863.7876470227845),\n",
       " (('every', 'thing'), 822.5115947376371),\n",
       " (('mr.', 'weston'), 819.316541417629),\n",
       " (('young', 'man'), 738.0043606265148),\n",
       " (('mr.', 'woodhouse'), 709.2509559335367),\n",
       " (('great', 'deal'), 624.5844244998653),\n",
       " (('maple', 'grove'), 543.5640164601741),\n",
       " (('mrs.', 'goddard'), 539.8815046356124),\n",
       " (('dare', 'say'), 519.4899438008467),\n",
       " (('john', 'knightley'), 481.1441515680198),\n",
       " (('miss', 'taylor'), 455.8587834632044),\n",
       " (('miss', 'smith'), 449.07602962315485),\n",
       " (('robert', 'martin'), 423.1323714473208),\n",
       " (('colonel', 'campbell'), 382.65688903898683),\n",
       " (('box', 'hill'), 290.2446133043724),\n",
       " (('depend', 'upon'), 239.06344414801313),\n",
       " (('upon', 'word'), 225.62889299685202),\n",
       " (('william', 'larkins'), 220.7772382638071),\n",
       " (('said', 'emma'), 210.3560331875389),\n",
       " (('mrs.', 'cole'), 199.17528994831108),\n",
       " (('mr.', 'martin'), 197.65324181025966),\n",
       " (('harriet', 'smith'), 196.81576555709933),\n",
       " (('mr.', 'perry'), 196.536981016899),\n",
       " (('mr.', 'frank'), 192.58441863725932),\n",
       " (('young', 'lady'), 186.30067367322556),\n",
       " (('brunswick', 'square'), 184.57018059042792),\n",
       " (('body', 'else'), 181.24121419336157),\n",
       " (('oh', 'yes'), 179.26479921776485),\n",
       " (('mrs.', 'churchill'), 179.19140524648583),\n",
       " (('take', 'care'), 168.81405687955515),\n",
       " (('miss', 'hawkins'), 165.86436927467986),\n",
       " (('young', 'woman'), 164.55525411114633),\n",
       " (('half', 'hour'), 157.83582405250291),\n",
       " (('last', 'night'), 155.02535338141118),\n",
       " (('young', 'ladies'), 144.1063007032337),\n",
       " (('cried', 'emma'), 140.56828248109935),\n",
       " (('mrs.', 'bates'), 136.46344246179257),\n",
       " (('mr.', 'john'), 134.25009943721378),\n",
       " (('good', 'deal'), 132.28131319146647),\n",
       " (('emma', 'could'), 130.83547258489395),\n",
       " (('mr.', 'dixon'), 128.0663309196494)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "word_fd = nltk.FreqDist(emma_cleaner) # all the words\n",
    "bigram_fd = nltk.FreqDist(nltk.bigrams(emma_cleaner))\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "scored = finder.score_ngrams(bigram_measures.likelihood_ratio) # a good option here, there are others:\n",
    "scored[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trigrams - using raw counts is much faster than using a statistical measure of likelihood\n",
    "\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(emma_cleaner,\n",
    "    window_size = 15)\n",
    "# there must be at least 2 for them to be reported:\n",
    "finder.apply_freq_filter(2)\n",
    "# if you want to remove extra words, like character names, you can create the ignored_words list too:\n",
    "#finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mr.', 'knightley', 'mr.'),\n",
       " ('mr.', 'elton', 'mr.'),\n",
       " ('mr.', 'mr.', 'knightley'),\n",
       " ('mr.', 'mr.', 'elton'),\n",
       " ('mr.', 'frank', 'churchill'),\n",
       " ('mr.', 'knightley', 'emma'),\n",
       " ('mrs.', 'weston', 'emma'),\n",
       " ('mr.', 'knightley', 'would'),\n",
       " ('emma', 'mr.', 'knightley'),\n",
       " ('mr.', 'weston', 'mr.'),\n",
       " ('mrs.', 'weston', 'would'),\n",
       " ('mr.', 'mrs.', 'weston'),\n",
       " ('could', 'mr.', 'knightley'),\n",
       " ('emma', 'mrs.', 'weston'),\n",
       " ('mr.', 'knightley', 'could'),\n",
       " ('mrs.', 'weston', 'mr.'),\n",
       " ('would', 'mr.', 'knightley'),\n",
       " ('elton', 'mr.', 'elton'),\n",
       " ('mr.', 'john', 'knightley'),\n",
       " ('mr.', 'mr.', 'weston')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# now we use the raw counts here and you'll see lots of garbage unless you did further cleaning\n",
    "finder.nbest(trigram_measures.raw_freq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mr.', 'knightley', 'mr.'), 1.5006160028691778e-05),\n",
       " (('mr.', 'elton', 'mr.'), 1.3355482425535682e-05),\n",
       " (('mr.', 'mr.', 'knightley'), 1.2605174424101094e-05),\n",
       " (('mr.', 'mr.', 'elton'), 1.1104558421231916e-05),\n",
       " (('mr.', 'frank', 'churchill'), 1.0954496820944997e-05),\n",
       " (('mr.', 'knightley', 'emma'), 1.0054127219223492e-05),\n",
       " (('mrs.', 'weston', 'emma'), 9.904065618936573e-06),\n",
       " (('mr.', 'knightley', 'would'), 9.45388081807582e-06),\n",
       " (('emma', 'mr.', 'knightley'), 9.303819217788902e-06),\n",
       " (('mr.', 'weston', 'mr.'), 9.003696017215066e-06),\n",
       " (('mrs.', 'weston', 'would'), 8.853634416928149e-06),\n",
       " (('mr.', 'mrs.', 'weston'), 8.10332641549356e-06),\n",
       " (('could', 'mr.', 'knightley'), 7.953264815206642e-06),\n",
       " (('emma', 'mrs.', 'weston'), 7.953264815206642e-06),\n",
       " (('mr.', 'knightley', 'could'), 7.953264815206642e-06),\n",
       " (('mrs.', 'weston', 'mr.'), 7.653141614632807e-06),\n",
       " (('would', 'mr.', 'knightley'), 7.653141614632807e-06),\n",
       " (('elton', 'mr.', 'elton'), 7.503080014345889e-06),\n",
       " (('mr.', 'john', 'knightley'), 7.503080014345889e-06),\n",
       " (('mr.', 'mr.', 'weston'), 7.503080014345889e-06)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.score_ngrams(trigram_measures.raw_freq)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is very slow!  Don't run unless you're serious :)\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(emma_cleaner,\n",
    "    window_size = 10)\n",
    "finder.apply_freq_filter(2)\n",
    "# if you want to remove extra words, like character names, you can create the ignored_words list too:\n",
    "#finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "#finder.apply_word_filter(lambda w: len(w) < 3)  # remove short words\n",
    "# maximum likelihood ratio is a statistical measure different from just raw counts used above\n",
    "finder.nbest(trigram_measures.likelihood_ratio, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more help is here: http://www.nltk.org/howto/collocations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we wanted to try non-fiction, to see if there are more interesting results?\n",
    "\n",
    "We need to read and clean the text for another file.  Let's try positive movie reviews, located in data/movie_reviews/all_pos.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"data/movie_reviews/all_pos.txt\", errors=\"ignore\") as handle:\n",
    "    text = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)  # tokenize them - split into words and punct\n",
    "clean_posrevs = clean_tokens(tokens, english_stops)  # clean up stopwords and punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rated',\n",
       " '4-star',\n",
       " 'scale',\n",
       " 'screening',\n",
       " 'venue',\n",
       " 'odoen',\n",
       " 'liverpool',\n",
       " 'city',\n",
       " 'centre',\n",
       " 'released',\n",
       " 'uk',\n",
       " 'uip',\n",
       " 'april',\n",
       " '7',\n",
       " '2000',\n",
       " 'certificate',\n",
       " '15',\n",
       " '126',\n",
       " 'minutes',\n",
       " 'country']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_posrevs[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('--', '--'), 276.37856128911676),\n",
       " (('martial', 'arts'), 172.9566013400519),\n",
       " (('green', 'mile'), 124.60518604561268),\n",
       " (('jackie', 'brown'), 114.33464649956649),\n",
       " (('high', 'fidelity'), 101.97315011972981),\n",
       " (('wonder', 'boys'), 91.96089134910119),\n",
       " (('mr', 'tarantino'), 87.87868213572972),\n",
       " (('http', '//www'), 78.12829220990892),\n",
       " (('pulp', 'fiction'), 78.12829220990892),\n",
       " (('mira', 'sorvino'), 75.91905463957946),\n",
       " (('beavis', 'butthead'), 70.98878226918048),\n",
       " (('hong', 'kong'), 70.98878226918048),\n",
       " (('irwin', 'winkler'), 70.98878226918048),\n",
       " (('mrs', 'pascal'), 70.98878226918048),\n",
       " (('ca', \"n't\"), 70.26383877215791),\n",
       " (('first', 'sight'), 69.93537990510472),\n",
       " (('good', 'hunting'), 69.39432459762817),\n",
       " (('replacement', 'killers'), 65.9847580337986),\n",
       " (('robin', 'williams'), 65.9402713911571),\n",
       " (('running', 'time'), 64.15271723747468),\n",
       " (('matt', 'damon'), 63.775616177967265),\n",
       " (('pam', 'grier'), 63.35061224964273),\n",
       " (('serial', 'killer'), 63.35061224964273),\n",
       " (('ben', 'affleck'), 58.62347389043332),\n",
       " (('erin', 'brockovich'), 58.34696880344971),\n",
       " (('quentin', 'tarantino'), 55.712442230104976),\n",
       " (('enter', 'dragon'), 54.96796464241866),\n",
       " (('kung', 'fu'), 54.96796464241866),\n",
       " (('van', 'sant'), 54.96796464241866),\n",
       " (('sight', '``'), 54.955604978126146),\n",
       " (('twists', 'turns'), 51.56549208743765),\n",
       " (('years', 'ago'), 51.431961982885824),\n",
       " (('de', 'niro'), 50.469283485472864),\n",
       " (('dinner', 'game'), 49.06561702343973),\n",
       " (('niagara', 'niagara'), 48.81111952999776),\n",
       " (('val', 'kilmer'), 48.2378479723261),\n",
       " (('new', 'york'), 47.71977026233416),\n",
       " (('john', 'cusack'), 46.91800191579502),\n",
       " (('kelly', 'mcgillis'), 46.65019847569932),\n",
       " (('bvoice', 'com'), 45.970792695925695),\n",
       " (('eddie', 'murphy'), 45.40725117661672),\n",
       " (('alien', 'resurrection'), 44.682234881296246),\n",
       " (('white', 'trash'), 44.38295283189096),\n",
       " (('record', 'store'), 43.73954756831499),\n",
       " (('redman', 'bvoice'), 43.73954756831499),\n",
       " (('``', 'eraserhead'), 43.45753554164391),\n",
       " (('eraserhead', '``'), 43.45753554164391),\n",
       " (('coen', 'brothers'), 41.50849284436478),\n",
       " (('stephen', 'frears'), 40.909331598042584),\n",
       " (('kirk', 'douglas'), 39.955891936273034)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_fd = nltk.FreqDist(clean_posrevs)\n",
    "bigram_fd = nltk.FreqDist(nltk.bigrams(clean_posrevs))\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "scored = finder.score_ngrams(bigram_measures.likelihood_ratio) # other options are \n",
    "scored[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently we need to remove custom stopwords from these too!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pandasnlp]",
   "language": "python",
   "name": "conda-env-pandasnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
