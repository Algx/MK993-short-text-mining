{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Multiple Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data problems with text involve dealing with multiple files.  You need a little bit of python to handle them.  We'll add a few utility functions to our collection in nlp_utilities.  The first is one to get the filenames from a directory of files, which is how your texts will usually be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_filenames(folder):\n",
    "    \"\"\" Pass in a folder name, with or without the / at end.\n",
    "    Returns a list of the files & paths inside it (no folders).\n",
    "    \"\"\"\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    # because we want to return full paths, we need to make sure there is\n",
    "    # a / at the end.\n",
    "    # If this doesn't work on Windows, change the slash direction.\n",
    "    if folder[-1:] != \"/\":\n",
    "        folder = folder + \"/\"\n",
    "    # this will return only the filenames, not folders inside the path\n",
    "    # also filter out .DS_Store which is on Macs.\n",
    "    return [folder + f for f in listdir(folder) if isfile(join(folder, f)) and f != \".DS_Store\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/books/Austen_Emma.txt',\n",
       " 'data/books/Austen_Pride.txt',\n",
       " 'data/books/edgar_allen_poe.txt',\n",
       " 'data/books/lovecraft.txt',\n",
       " 'data/books/Melville_MobyDick.txt',\n",
       " 'data/books/mrjames_ghoststories.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = get_filenames(\"data/books/\")\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read in the text from each of these files, we will create a dictionary, with the key being the filename and the value being the text (not tokens yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_texts_as_string(filenames):\n",
    "    \"\"\" Takes a list of filenames as arg.\n",
    "    Returns a dictionary with filename as key, string as value.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    loaded_text = defaultdict(str)  # each value is a string, the text\n",
    "    for filename in filenames:\n",
    "        with open(filename, errors=\"ignore\") as handle:\n",
    "            loaded_text[filename] = handle.read()\n",
    "    return loaded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of running it on one file - it requires a list as input, so we wrap one of the filename items in [] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load_texts_as_string([filenames[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So to get the texts for each filename in the folder, we pass in the filenames we just collected:\n",
    "\n",
    "texts = load_texts_as_string(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data/books/Melville_MobyDick.txt', 'data/books/lovecraft.txt', 'data/books/mrjames_ghoststories.txt', 'data/books/Austen_Emma.txt', 'data/books/Austen_Pride.txt', 'data/books/edgar_allen_poe.txt'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here is our list of files as keys:\n",
    "texts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to use this as a real list, we have to wrap it with the function `list()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/books/Melville_MobyDick.txt',\n",
       " 'data/books/lovecraft.txt',\n",
       " 'data/books/mrjames_ghoststories.txt',\n",
       " 'data/books/Austen_Emma.txt',\n",
       " 'data/books/Austen_Pride.txt',\n",
       " 'data/books/edgar_allen_poe.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(texts.keys()) # [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And to get the values, the text strings, it's\n",
    "\n",
    "`texts.values()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                        The\\n                             Shunned House\\n\\n                        By '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also return the value using its key, of course:\n",
    "texts['data/books/lovecraft.txt'][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are some more utility functions and cleanup functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we don't want numbers, like chapter numbers or dates in our vocabulary. This function removes any token that doesn't have a letter in it, using python's regular expression search syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the string 12: None\n",
      "Searching the string time12 <_sre.SRE_Match object; span=(0, 1), match='t'>\n",
      "Searching the string 3rd <_sre.SRE_Match object; span=(1, 2), match='r'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# the regular expression [a-zA-Z] means any letter in the alphabet,\n",
    "# upper or lower case.  If we match against digits, there will be\n",
    "# no hit.\n",
    "print(\"Searching the string 12:\", re.search('[a-zA-Z]', '12'))\n",
    "print(\"Searching the string time12\", re.search('[a-zA-Z]', 'time12'))\n",
    "print(\"Searching the string 3rd\", re.search('[a-zA-Z]', '3rd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_nonletters(wordlist):\n",
    "    \"\"\" \n",
    "    Checks for letters in the token - using a regex search.\n",
    "    Strings that are just numbers or remaining punctuation will \n",
    "    be removed! No more custom stop '--'. But ''s' will remain.\n",
    "    \"\"\"\n",
    "    return [word for word in wordlist if re.search('[a-zA-Z]', word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update our cleaning function with this too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    \"\"\" Takes a list of tokens. Returns lowercased, minus punct \n",
    "    and stopwords and digits.\n",
    "    \"\"\"\n",
    "    words = remove_punct(tokens)\n",
    "    words = remove_stops(words)\n",
    "    words = remove_nonletters(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function to stem tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a function that will tokenize, clean, and stem our text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_clean_stem(text):\n",
    "    # takes an already read text and tokenizes stems and clean\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = clean_tokens(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use that, we need to import the file nlp_utilities.py that has all the other definitions (along with these new ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nlp_utilities as mytools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the functions available on this package name, in the juypyter notebook you can type the \".\" and then a tab. You will get a little popup menu!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-333308f82d15>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-333308f82d15>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mytools.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mytools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = mytools.tokenize_clean_stem(texts['data/books/lovecraft.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shun',\n",
       " 'hous',\n",
       " 'h.',\n",
       " 'p.',\n",
       " 'lovecraft',\n",
       " 'even',\n",
       " 'greatest',\n",
       " 'horror',\n",
       " 'ironi',\n",
       " 'seldom',\n",
       " 'absent',\n",
       " 'sometim',\n",
       " 'enter',\n",
       " 'directli',\n",
       " 'composit',\n",
       " 'event',\n",
       " 'sometim',\n",
       " 'relat',\n",
       " 'fortuit',\n",
       " 'posit']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens2 = mytools.tokenize_clean(texts['data/books/lovecraft.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shunned',\n",
       " 'house',\n",
       " 'h.',\n",
       " 'p.',\n",
       " 'lovecraft',\n",
       " 'even',\n",
       " 'greatest',\n",
       " 'horrors',\n",
       " 'irony',\n",
       " 'seldom',\n",
       " 'absent',\n",
       " 'sometimes',\n",
       " 'enters',\n",
       " 'directly',\n",
       " 'composition',\n",
       " 'events',\n",
       " 'sometimes',\n",
       " 'relates',\n",
       " 'fortuitous',\n",
       " 'position']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                        The\\n                             Shunned House\\n\\n                        By H. P. LOVECRAFT\\n\\n\\n\\nFrom even the greatest of horrors irony is seldom absent. Sometimes it\\nenters dir'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts['data/books/lovecraft.txt'][0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Guide\n",
    "\n",
    "While we're talking about code, this is PEP8: the Python style guidelines. If you ever interview for a job using Python, you should be quite familiar with it. https://www.python.org/dev/peps/pep-0008/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pandasnlp]",
   "language": "python",
   "name": "conda-env-pandasnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
